{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4526bafa-3a56-4080-900a-40c7a0cc1cf0",
   "metadata": {},
   "source": [
    "# Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a76aef-5874-44d6-8bc4-3c343a3e49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import uuid\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d09ac06-7ea5-4f31-8185-6474cfbca502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209527 entries, 0 to 209526\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   uuid               209527 non-null  object        \n",
      " 1   link               209527 non-null  object        \n",
      " 2   headline           209527 non-null  object        \n",
      " 3   category           209527 non-null  object        \n",
      " 4   short_description  209527 non-null  object        \n",
      " 5   authors            209527 non-null  object        \n",
      " 6   date               209527 non-null  datetime64[ns]\n",
      " 7   clean_headline     209527 non-null  object        \n",
      " 8   combined_text      209527 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(8)\n",
      "memory usage: 14.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load in the data and add uuids \n",
    "data = pd.read_json(\"data/preprocessed_data.jsonl\", orient='records', lines=True)\n",
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec434b-1006-4bb8-b8a6-cb9afb38b437",
   "metadata": {},
   "source": [
    "In order to help speed up and simplify some of the code rather then relying on pandas for looking up articles I will use dictionaries instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cefba9c-a280-4170-a470-0e7da0931db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lookups including uuid -> title and row index -> uuid\n",
    "UUID_2_TITLE = dict(zip(data['uuid'], data['headline']))\n",
    "IND_2_UUID = dict(zip(list(data.index), data['uuid']))\n",
    "\n",
    "# Create generic docs list to be used for setting up searchs. \n",
    "DOCS = list(data['combined_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3350ab1-059a-4aef-aabc-a18bb577fc78",
   "metadata": {},
   "source": [
    "## Naive Approach \n",
    "\n",
    "In the naive approach we will simply check each document to see if it contains the query term/s. The main reason for calling this the \"naive\" approach is because it will utilize a for loop to check each document for the term/s. In a small sample such as this using a simple for loop works relatively quickly while very clearly showing how keyword matching works however with larger datasets with not only more records but also longer texts this approach is simply unrealistic due to its limitation in regards to processing time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef82fcb-e53d-45b2-8269-bffc063fa2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query:str, docs:list)->List[str]:\n",
    "    matches = [d for d in docs if query.lower() in d]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b9ab10-c86d-4354-b8f1-87c2d7543ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396\n",
      "CPU times: user 305 ms, sys: 0 ns, total: 305 ms\n",
      "Wall time: 275 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = search(query=\"airlines\", docs=DOCS)\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862f6587-1771-442f-83e9-86674dd7170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american airlines flyer charged banned for life after punching flight attendant on video he was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation according to the us attorney's office in los angeles\n",
      "\n",
      "alaska airlines cancels dozens of flights as pilots picket more than 100 alaska airlines flights were canceled by the airline including 66 in seattle 20 in portland oregon 10 in los angeles and seven in san francisco\n",
      "\n",
      "russia's flagship airline aeroflot halts all international flights except to belarus russia's aviation agency recommended all russian airlines with foreignleased planes halt both passenger and cargo flights abroad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in results[:3]:\n",
    "    print(r + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76d4b1-d2dc-4713-9853-7c4da5578cfc",
   "metadata": {},
   "source": [
    "As can be seen this performs well enough in regards to surfacing documents that contain the search term (i.e. \"airlines\") however as noted this approach would not be feasible on larger datasets and also comes into issues when dealing with longer search terms. In other words once the query gets longer (e.g. \"airlines in the united states\") this will introduce to many complexities to really be handled via a simple term lookup because then you may need to start testing strategies that split up the query into individual terms and search against each word at a time which again leads to poor performance in terms of lookup speeds. Therefore in an attempt to try and solve for this we will perform keyword search against vectorized documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc509e-e8a5-45ee-9fcf-6f46cd692c07",
   "metadata": {},
   "source": [
    "## Vectorized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4126810e-b365-4375-b602-29d50e69d531",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2db89a-1db0-445b-9130-d3007bbd279b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86caf0c-a9fa-433e-bda1-36888f5c8dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112dbdf7-91cb-4bd0-bdc8-40a3033c5b07",
   "metadata": {},
   "source": [
    "### tfidf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bf669-6a2f-40fa-bf79-331992efcaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
